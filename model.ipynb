{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import deque \n",
    "\n",
    "from model import HDDriveDQN, HDMapSensorDQN, hd_net_args\n",
    "from agent import MemoryBufferSimple\n",
    "import random\n",
    "import itertools\n",
    "from srunner.tools import dotdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['frame', 'accelerometer', 'gyroscope', 'compass', 'gnss', 'velocity', 'hd_map', 'front_rgb'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('state.pickle', 'rb') as fp:\n",
    "    state = pickle.load(fp)\n",
    "state.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_size = 128\n",
    "hd_sensor_model = HDMapSensorDQN((3,96,96), 7, p=0.3)\n",
    "npImg_to_tensor = lambda x: torch.tensor(np.expand_dims(x.transpose(2,0,1), axis=0), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 500, 400]),\n",
       " torch.Size([1, 3, 500, 400]),\n",
       " torch.Size([1, 3]),\n",
       " torch.Size([1, 3]),\n",
       " torch.Size([1, 1]),\n",
       " torch.Size([1]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_X = torch.tensor(state['accelerometer'], dtype=torch.float32).unsqueeze(0)\n",
    "comp_X = torch.tensor(state['compass'], dtype=torch.float32).unsqueeze(0)\n",
    "gyro_X = torch.tensor(state['gyroscope'], dtype=torch.float32).unsqueeze(0)\n",
    "bev_X = npImg_to_tensor(state['hd_map'])\n",
    "front_X = npImg_to_tensor(state['front_rgb'])\n",
    "vel_X = torch.tensor(state['velocity']).unsqueeze(0)\n",
    "bev_X.shape, front_X.shape, acc_X.shape, gyro_X.shape, comp_X.shape, vel_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32x49600 and 2304x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m bev_X_h, front_X_h, ego_X_h, vel_X_h \u001b[39m=\u001b[39m hd_sensor_model(bev_X\u001b[39m.\u001b[39;49mrepeat(\u001b[39m32\u001b[39;49m,\u001b[39m1\u001b[39;49m,\u001b[39m1\u001b[39;49m,\u001b[39m1\u001b[39;49m), front_X\u001b[39m.\u001b[39;49mrepeat(\u001b[39m32\u001b[39;49m,\u001b[39m1\u001b[39;49m,\u001b[39m1\u001b[39;49m,\u001b[39m1\u001b[39;49m), acc_X\u001b[39m.\u001b[39;49mrepeat(\u001b[39m32\u001b[39;49m,\u001b[39m1\u001b[39;49m), comp_X\u001b[39m.\u001b[39;49mrepeat(\u001b[39m32\u001b[39;49m,\u001b[39m1\u001b[39;49m), gyro_X\u001b[39m.\u001b[39;49mrepeat(\u001b[39m32\u001b[39;49m,\u001b[39m1\u001b[39;49m), vel_X\u001b[39m.\u001b[39;49mrepeat(\u001b[39m32\u001b[39;49m))\n\u001b[0;32m      2\u001b[0m bev_X_h\u001b[39m.\u001b[39mshape, front_X_h\u001b[39m.\u001b[39mshape, ego_X_h\u001b[39m.\u001b[39mshape, vel_X_h\u001b[39m.\u001b[39mshape\n",
      "File \u001b[1;32mc:\\Users\\test\\Documents\\driving\\rl_ad\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\test\\Documents\\driving\\rl_ad\\scenario_runner_\\model\\sensor_net.py:32\u001b[0m, in \u001b[0;36mHDMapSensorDQN.forward\u001b[1;34m(self, bev_X, front_X, acc_X, comp_X, gyro_X, vel_X)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, bev_X, front_X, acc_X, comp_X, gyro_X, vel_X):\n\u001b[1;32m---> 32\u001b[0m     bev_X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbev_dense(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbev_cnn(bev_X))\u001b[39m.\u001b[39munsqueeze(dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m     33\u001b[0m     front_X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbev_dense(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbev_cnn(front_X))\u001b[39m.\u001b[39munsqueeze(dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m     34\u001b[0m     ego_X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mego_dense(torch\u001b[39m.\u001b[39mconcat([acc_X, comp_X, gyro_X], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m))\u001b[39m.\u001b[39munsqueeze(dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\test\\Documents\\driving\\rl_ad\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\test\\Documents\\driving\\rl_ad\\env\\lib\\site-packages\\torch\\nn\\modules\\container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\test\\Documents\\driving\\rl_ad\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\test\\Documents\\driving\\rl_ad\\env\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x49600 and 2304x128)"
     ]
    }
   ],
   "source": [
    "bev_X_h, front_X_h, ego_X_h, vel_X_h = hd_sensor_model(bev_X.repeat(32,1,1,1), front_X.repeat(32,1,1,1), acc_X.repeat(32,1), comp_X.repeat(32,1), gyro_X.repeat(32,1), vel_X.repeat(32))\n",
    "bev_X_h.shape, front_X_h.shape, ego_X_h.shape, vel_X_h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = hd_net_args\n",
    "hd_drive_net = HDDriveDQN(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 32, 3, 96, 96])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bev_X.repeat(4,32,1,1,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 128])\n",
      "torch.Size([4, 32, 128])\n"
     ]
    }
   ],
   "source": [
    "steering_q_vals, throttle_q_vals, brake_q_vals, t_or_b_vals = hd_drive_net(bev_X.repeat(args.n_frames,32,1,1,1), \n",
    "             front_X.repeat(args.n_frames, 32,1,1,1), \n",
    "             acc_X.repeat(args.n_frames, 32,1), \n",
    "             comp_X.repeat(args.n_frames, 32,1), \n",
    "             gyro_X.repeat(args.n_frames, 32,1), \n",
    "             vel_X.repeat(args.n_frames, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 11]), torch.Size([32, 11]), torch.Size([32, 11]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steering_q_vals.shape, throttle_q_vals.shape, brake_q_vals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(750784, 661632, 661632, 1419, 1419, 1419, 258)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensor_params = sum(p.numel() for p in hd_drive_net.sensor_net.parameters())\n",
    "fusion_params = sum(p.numel() for p in hd_drive_net.fusion_net.parameters())\n",
    "temporal_params = sum(p.numel() for p in hd_drive_net.temporal_net.parameters())\n",
    "throttle_params = sum(p.numel() for p in hd_drive_net.throttle_net.parameters())\n",
    "steering_params = sum(p.numel() for p in hd_drive_net.steering_net.parameters())\n",
    "brake_params = sum(p.numel() for p in hd_drive_net.brake_net.parameters())\n",
    "t_or_b_params = sum(p.numel() for p in hd_drive_net.t_or_b_net.parameters())\n",
    "sensor_params, fusion_params, temporal_params, throttle_params, steering_params, brake_params, t_or_b_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2078563"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in hd_drive_net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryBufferSeparated(torch.utils.data.Dataset):\n",
    "    \"\"\"Assumme that each episode has at least num_frames+1 experieneces\n",
    "       else it will be discarded. When max is reached max(10% samples,1ep) will be dropped.\"\"\"\n",
    "    def __init__(self, num_frames, max_buffer_sz=25_000):\n",
    "        self.avg_ep_len = None \n",
    "        self.ep_lengths = deque()\n",
    "        self.max_buffer_size = max_buffer_sz\n",
    "        self.ep_states = deque()\n",
    "        self.ep_actions = deque()\n",
    "        self.ep_rewards = deque()\n",
    "        self.ep_dones = deque()\n",
    "        self.active_ep_idx = -1 \n",
    "        self.num_frames = num_frames\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.states) - self.num_frames#sum(self.ep_lengths)\n",
    "\n",
    "    def add_experience(self, state, action, reward, done, new_episode=False):\n",
    "        \"\"\"states are at t+1, so when get_item we have to select actions, rewards one forward\"\"\"\n",
    "        if new_episode:\n",
    "            self.ep_lengths.append(0)\n",
    "            self.ep_states.append([])\n",
    "            self.ep_actions.append([])\n",
    "            self.ep_rewards.append([])\n",
    "            self.ep_dones.append([])\n",
    "\n",
    "        self.ep_lengths[-1] += 1 \n",
    "        if self.max_buffer_size < sum(self.ep_lengths):\n",
    "            self.flush_experiences()\n",
    "\n",
    "        self.ep_states[-1].append(state)\n",
    "        self.ep_actions[-1].append(action)\n",
    "        self.ep_rewards[-1].append(reward)\n",
    "        self.ep_dones[-1].append(done)\n",
    "\n",
    "    def flush_experiences(self, per=0.1):\n",
    "        to_flush = int(per*self.max_buffer_size)\n",
    "        flushed = 0\n",
    "        while flushed < to_flush:\n",
    "            flushed += self.ep_lengths.popleft()\n",
    "            self.ep_states.popleft()\n",
    "            self.ep_actions.popleft()\n",
    "            self.ep_rewards.popleft()\n",
    "            self.ep_dones.popleft()\n",
    "        \n",
    "    def _np_img_to_tensor(self, img):\n",
    "        img = np.array(img) # num_frames+1 x W x H x C\n",
    "        return torch.tensor(np.expand_dims(img.transpose(0,3,1,2), axis=0), dtype=torch.float32)\n",
    "\n",
    "    def _process_states(self, state):\n",
    "        \"\"\"This function should be passed into the class as it could depend on the\n",
    "           sensor setup. This configuration works for the HDSensor setup.\"\"\"\n",
    "        bev_X = self._np_img_to_tensor(state['hd_map'])\n",
    "        front_X = self._np_img_to_tensor(state['front_rgb'])\n",
    "        acc_x = torch.tensor(state['accelerometer'], dtype=torch.float32)\n",
    "        comp_x = torch.tensor(state['compass'], dtype=torch.float32)\n",
    "        gyr_x = torch.tensor(state['gyroscope'], dtype=torch.float32) \n",
    "        vel_X = torch.tensor(state['velocity']).unsqueeze(0)\n",
    "\n",
    "        return bev_X, front_X, acc_X, comp_X, gyr_x, vel_X\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        ep_idx = random.choice(range(len(self.ep_lengths)-1)) # don't sample from current ep as it might be filled out now\n",
    "        t_idx = random.choice(range(self.ep_lengths[ep_idx]-self.num_frames-1)) # -1 as we need s_t+1 also\n",
    "        end_idx = t_idx + self.num_frames + 1 # +1 as we need next state as well\n",
    "        states = self._process_states(self.ep_states[ep_idx][t_idx:end_idx])\n",
    "        action = torch.tensor(self.ep_actions[ep_idx][end_idx], dtype=torch.int64) \n",
    "        reward = torch.tensor(self.ep_rewards[ep_idx][end_idx], dtype=torch.float32)\n",
    "        dones = torch.tensor(self.ep_rewards[ep_idx][end_idx], dtype=torch.float32)\n",
    "        return states, action, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = MemoryBufferSimple(num_frames=4, max_buffer_sz=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buffer.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "for i in range(n):\n",
    "    buffer.add_experience(state, [0,0,0], 1.0, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(buffer, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 5, 3, 96, 96]), torch.Size([16, 3]), torch.Size([16]))"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = 'cpu'\n",
    "state_batch, action_batch, reward_batch = x\n",
    "state_batch[0].shape, action_batch.shape, reward_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(30):\n",
    "    next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 5, 3, 96, 96]), torch.Size([16, 5, 3, 96, 96]))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0][0], x[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0][0].device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 5, 3, 96, 96])\n",
      "torch.Size([5, 5, 3, 96, 96])\n",
      "torch.Size([5, 5, 3])\n",
      "torch.Size([5, 5, 1])\n",
      "torch.Size([5, 5, 3])\n",
      "torch.Size([5, 5])\n"
     ]
    }
   ],
   "source": [
    "for state_tensor in x[0]:\n",
    "    state_tensor.transpose_(0,1)\n",
    "    print(state_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 128])\n",
      "torch.Size([4, 5, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 11]),\n",
       " torch.Size([5, 11]),\n",
       " torch.Size([5, 11]),\n",
       " torch.Size([5, 2]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_state = x[0][0][:-1], x[0][1][:-1], x[0][2][:-1], x[0][3][:-1], x[0][4][:-1], x[0][5][:-1]\n",
    "s_q, t_q, b_q, t_or_b_q = hd_drive_net(*cur_state)\n",
    "s_q.shape, t_q.shape, b_q.shape, t_or_b_q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 5, 3, 96, 96])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_state[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5, 3, 96, 96])\n",
      "torch.Size([4, 5, 3, 96, 96])\n",
      "torch.Size([4, 5, 3])\n",
      "torch.Size([4, 5, 1])\n",
      "torch.Size([4, 5, 3])\n",
      "torch.Size([4, 5])\n"
     ]
    }
   ],
   "source": [
    "for state_tensor in cur_state:\n",
    "    print(state_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drive_env",
   "language": "python",
   "name": "drive_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "205595cd1daa4a192140885c104c4ea416cf31c31e44558713cd0df72bb67f19"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
